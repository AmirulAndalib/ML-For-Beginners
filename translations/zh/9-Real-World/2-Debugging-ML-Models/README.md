<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ba0f6e1019351351c8ee4c92867b6a0b",
  "translation_date": "2025-09-03T17:30:13+00:00",
  "source_file": "9-Real-World/2-Debugging-ML-Models/README.md",
  "language_code": "zh"
}
-->
# 后记：使用负责任的AI仪表板组件进行机器学习模型调试

## [课前测验](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)

## 简介

机器学习正在影响我们的日常生活。人工智能正在逐步进入一些对个人和社会至关重要的系统，例如医疗、金融、教育和就业领域。例如，系统和模型参与了日常决策任务，如医疗诊断或欺诈检测。因此，随着人工智能的快速发展和广泛应用，社会对其的期望也在不断变化，同时相关法规也在逐步完善。我们经常看到人工智能系统未能达到预期的领域，它们暴露出新的挑战，而各国政府也开始对人工智能解决方案进行监管。因此，分析这些模型以确保其为所有人提供公平、可靠、包容、透明和负责任的结果是非常重要的。

在本课程中，我们将探讨一些实用工具，用于评估模型是否存在负责任的人工智能问题。传统的机器学习调试技术通常基于定量计算，例如整体准确率或平均误差损失。然而，想象一下，当您用于构建这些模型的数据缺乏某些人口统计信息（如种族、性别、政治观点、宗教）或这些人口统计信息被不成比例地代表时会发生什么情况。如果模型的输出被解释为偏向某些人口统计信息，这可能会导致这些敏感特征组的过度或不足代表，从而引发模型的公平性、包容性或可靠性问题。此外，机器学习模型通常被认为是“黑箱”，这使得理解和解释模型预测的驱动因素变得困难。这些都是数据科学家和人工智能开发者在缺乏足够工具来调试和评估模型的公平性或可信度时面临的挑战。

在本课程中，您将学习如何通过以下方式调试模型：

- **错误分析**：识别模型在数据分布中错误率较高的区域。
- **模型概览**：对不同数据群体进行比较分析，发现模型性能指标中的差异。
- **数据分析**：调查数据是否存在过度或不足代表性，这可能导致模型偏向某些数据群体。
- **特征重要性**：了解哪些特征在全局或局部层面驱动模型的预测。

## 前提条件

作为前提条件，请先查看[开发者的负责任AI工具](https://www.microsoft.com/ai/ai-lab-responsible-ai-dashboard)

> ![负责任AI工具的动图](../../../../9-Real-World/2-Debugging-ML-Models/images/rai-overview.gif)

## 错误分析

用于衡量准确性的传统模型性能指标通常基于正确与错误预测的计算。例如，确定一个模型89%的时间是准确的，误差损失为0.001，可以被认为是良好的性能。然而，错误通常不会在底层数据集中均匀分布。您可能获得89%的模型准确率，但发现模型在某些数据区域的失败率高达42%。这些特定数据组的失败模式可能导致公平性或可靠性问题。因此，了解模型表现良好或不佳的区域至关重要。模型在某些数据区域的高错误率可能表明这些数据组是重要的人口统计信息。

![分析和调试模型错误](../../../../translated_images/ea-error-distribution.117452e1177c1dd84fab2369967a68bcde787c76c6ea7fdb92fcf15d1fce8206.zh.png)

RAI仪表板上的错误分析组件通过树状可视化展示模型失败在不同群体中的分布情况。这有助于识别数据集中错误率较高的特征或区域。通过查看模型大部分错误的来源，您可以开始调查根本原因。您还可以创建数据群体以进行分析。这些数据群体有助于调试过程，以确定为什么模型在一个群体中表现良好，而在另一个群体中却出现错误。

![错误分析](../../../../translated_images/ea-error-cohort.6886209ea5d438c4daa8bfbf5ce3a7042586364dd3eccda4a4e3d05623ac702a.zh.png)

树状图上的视觉指示器可以更快地定位问题区域。例如，树节点的红色阴影越深，错误率越高。

热图是另一种可视化功能，用户可以使用一个或两个特征来调查错误率，从而发现整个数据集或群体中导致模型错误的因素。

![错误分析热图](../../../../translated_images/ea-heatmap.8d27185e28cee3830c85e1b2e9df9d2d5e5c8c940f41678efdb68753f2f7e56c.zh.png)

使用错误分析的场景：

* 深入了解模型错误在数据集和多个输入及特征维度上的分布。
* 分解整体性能指标，自动发现错误群体，以指导有针对性的缓解措施。

## 模型概览

评估机器学习模型的性能需要全面了解其行为。这可以通过查看多个指标（如错误率、准确率、召回率、精确度或平均绝对误差（MAE））来实现，以发现性能指标中的差异。一个性能指标可能看起来很好，但另一个指标可能暴露出不准确性。此外，比较整个数据集或群体的指标差异有助于揭示模型表现良好或不佳的区域。这对于观察模型在敏感特征（如患者种族、性别或年龄）与非敏感特征之间的表现尤为重要，以发现模型可能存在的潜在不公平性。例如，发现模型在包含敏感特征的群体中错误率更高可能揭示模型的潜在不公平性。

RAI仪表板的模型概览组件不仅有助于分析数据群体中的性能指标，还为用户提供了比较模型在不同群体中的行为的能力。

![数据群体 - RAI仪表板中的模型概览](../../../../translated_images/model-overview-dataset-cohorts.dfa463fb527a35a0afc01b7b012fc87bf2cad756763f3652bbd810cac5d6cf33.zh.png)

组件的基于特征的分析功能允许用户在特定特征内缩小数据子组，以更细粒度地识别异常。例如，仪表板具有内置智能，可以自动为用户选择的特征生成群体（例如，*"time_in_hospital < 3"* 或 *"time_in_hospital >= 7"*）。这使用户能够从较大的数据组中隔离特定特征，以查看它是否是模型错误结果的关键影响因素。

![特征群体 - RAI仪表板中的模型概览](../../../../translated_images/model-overview-feature-cohorts.c5104d575ffd0c80b7ad8ede7703fab6166bfc6f9125dd395dcc4ace2f522f70.zh.png)

模型概览组件支持两类差异指标：

**模型性能差异**：这些指标计算所选性能指标在数据子组之间的差异（差距）。以下是一些示例：

* 准确率差异
* 错误率差异
* 精确度差异
* 召回率差异
* 平均绝对误差（MAE）差异

**选择率差异**：此指标包含子组之间选择率（有利预测）的差异。例如，贷款批准率的差异。选择率指的是每个类别中被分类为1的数据点比例（在二元分类中）或预测值的分布（在回归中）。

## 数据分析

> “如果你对数据施加足够的压力，它会承认任何事情” - Ronald Coase

这句话听起来极端，但确实如此，数据可以被操纵以支持任何结论。这种操纵有时可能是无意的。作为人类，我们都有偏见，而我们往往难以意识到自己何时在数据中引入了偏见。确保人工智能和机器学习的公平性仍然是一个复杂的挑战。

数据是传统模型性能指标的一个巨大盲点。您可能有很高的准确率，但这并不总是反映数据集中可能存在的底层数据偏差。例如，如果一个公司员工数据集中有27%的女性在高管职位，而男性占73%，那么基于此数据训练的职位广告AI模型可能会主要针对男性观众投放高级职位广告。这种数据不平衡导致模型预测偏向某一性别。这揭示了模型存在性别偏见的公平性问题。

RAI仪表板上的数据分析组件有助于识别数据集中过度和不足代表的区域。它帮助用户诊断由于数据不平衡或缺乏特定数据组代表性而引入的错误和公平性问题的根本原因。这使用户能够根据预测和实际结果、错误组以及特定特征来可视化数据集。有时发现一个代表性不足的数据组也可能揭示模型学习效果不佳，从而导致高错误率。一个具有数据偏差的模型不仅是一个公平性问题，还表明模型不够包容或可靠。

![RAI仪表板上的数据分析组件](../../../../translated_images/dataanalysis-cover.8d6d0683a70a5c1e274e5a94b27a71137e3d0a3b707761d7170eb340dd07f11d.zh.png)

使用数据分析的场景：

* 通过选择不同的过滤器探索数据集统计信息，将数据切分为不同维度（也称为群体）。
* 了解数据集在不同群体和特征组中的分布。
* 确定与公平性、错误分析和因果关系相关的发现（来自其他仪表板组件）是否是数据集分布的结果。
* 决定在哪些领域收集更多数据，以缓解因代表性问题、标签噪声、特征噪声、标签偏差等因素导致的错误。

## 模型可解释性

机器学习模型往往是“黑箱”。理解哪些关键数据特征驱动模型的预测可能具有挑战性。提供模型为何做出某种预测的透明性非常重要。例如，如果一个AI系统预测某位糖尿病患者在30天内可能再次入院，它应该能够提供支持其预测的数据。这些支持数据指标可以为临床医生或医院提供透明性，以便做出明智的决策。此外，能够解释模型为何对某位患者做出预测有助于符合健康法规的问责要求。当您使用机器学习模型影响人们的生活时，理解和解释模型行为的驱动因素至关重要。模型可解释性和可理解性可以在以下场景中回答问题：

* 模型调试：为什么我的模型会犯这个错误？我如何改进模型？
* 人机协作：我如何理解并信任模型的决策？
* 法规合规：我的模型是否满足法律要求？

RAI仪表板的特征重要性组件帮助您调试并全面了解模型如何做出预测。它也是机器学习专业人士和决策者解释和展示影响模型行为的特征证据的有用工具，以满足法规要求。接下来，用户可以探索全局和局部解释，验证哪些特征驱动模型的预测。全局解释列出影响模型整体预测的主要特征。局部解释显示哪些特征导致模型对个别案例的预测。评估局部解释的能力在调试或审计特定案例时也很有帮助，以更好地理解和解释模型为何做出准确或不准确的预测。

![RAI仪表板的特征重要性组件](../../../../translated_images/9-feature-importance.cd3193b4bba3fd4bccd415f566c2437fb3298c4824a3dabbcab15270d783606e.zh.png)

* 全局解释：例如，哪些特征影响糖尿病患者入院模型的整体行为？
* 局部解释：例如，为什么一位年龄超过60岁且有过住院记录的糖尿病患者被预测为会或不会在30天内再次入院？

在调试模型性能的过程中，特征重要性可以显示某个特征在不同群体中的影响程度。它有助于揭示特征在驱动模型错误预测时的异常情况。特征重要性组件可以显示某个特征中的值是如何正面或负面影响模型结果的。例如，如果模型做出了错误预测，该组件可以让您深入分析并确定哪些特征或特征值驱动了预测。这种细节不仅有助于调试，还在审计场景中提供了透明性和问责性。最后，该组件可以帮助您识别公平性问题。例如，如果某个敏感特征（如种族或性别）在驱动模型预测中具有高度影响力，这可能表明模型存在种族或性别偏见。

![特征重要性](../../../../translated_images/9-features-influence.3ead3d3f68a84029f1e40d3eba82107445d3d3b6975d4682b23d8acc905da6d0.zh.png)

使用可解释性的场景：

* 通过了解哪些特征对预测最重要，确定您的AI系统预测的可信度。
* 通过首先理解模型并识别模型是否使用健康特征或仅仅是错误关联来调试模型。
* 通过了解模型是否基于敏感特征或与敏感特征高度相关的特征做出预测，揭示潜在的不公平性来源。
* 通过生成局部解释来展示模型决策的结果，建立用户对模型决策的信任。
* 完成AI系统的法规审计，以验证模型并监控模型决策对人类的影响。

## 结论

RAI仪表板的所有组件都是实用工具，帮助您构建对社会危害更小、可信度更高的机器学习模型。它有助于防止对人权的威胁；避免对某些群体的歧视或排除生活机会；以及减少身体或心理伤害的风险。它还通过生成局部解释来展示模型决策的结果，从而建立对模型决策的信任。一些潜在的危害可以分类为：

- **分配问题**，例如某个性别或种族被优待于另一个。
- **服务质量**。如果您仅为一个特定场景训练数据，而现实情况更复杂，这会导致服务表现不佳。
- **刻板印象**。将某个群体与预设属性联系起来。
- **贬低**。不公平地批评或标记某事或某人。
- **过度或不足的代表性**。这个概念指的是某些群体在某些职业中未被看到，而任何持续推动这种现象的服务或功能都可能造成伤害。

### Azure RAI 仪表板

[Azure RAI 仪表板](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu) 基于由领先学术机构和组织（包括微软）开发的开源工具构建。这些工具对数据科学家和 AI 开发者来说至关重要，帮助他们更好地理解模型行为，发现并缓解 AI 模型中的不良问题。

- 通过查看 RAI 仪表板的[文档](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu)，学习如何使用不同的组件。

- 查看一些 RAI 仪表板的[示例笔记本](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks)，以便在 Azure 机器学习中调试更负责任的 AI 场景。

---
## 🚀 挑战

为了从一开始就避免引入统计或数据偏差，我们应该：

- 确保参与系统开发的人员具有多样化的背景和观点
- 投资于反映社会多样性的数据集
- 开发更好的方法来检测和纠正偏差

思考现实生活中模型构建和使用中显现不公平的场景。我们还应该考虑什么？

## [课后测验](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/6/)
## 复习与自学

在本课中，你学习了一些将负责任的 AI 融入机器学习的实用工具。

观看以下工作坊以更深入地了解相关主题：

- 负责任的 AI 仪表板：由 Besmira Nushi 和 Mehrnoosh Sameki 主讲，实践中实现 RAI 的一站式解决方案

[![负责任的 AI 仪表板：实践中实现 RAI 的一站式解决方案](https://img.youtube.com/vi/f1oaDNl3djg/0.jpg)](https://www.youtube.com/watch?v=f1oaDNl3djg "负责任的 AI 仪表板：实践中实现 RAI 的一站式解决方案")

> 🎥 点击上方图片观看视频：负责任的 AI 仪表板：实践中实现 RAI 的一站式解决方案，由 Besmira Nushi 和 Mehrnoosh Sameki 主讲

参考以下材料，了解更多关于负责任的 AI 以及如何构建更值得信赖的模型：

- 微软的 RAI 仪表板工具，用于调试机器学习模型：[负责任的 AI 工具资源](https://aka.ms/rai-dashboard)

- 探索负责任的 AI 工具包：[Github](https://github.com/microsoft/responsible-ai-toolbox)

- 微软的 RAI 资源中心：[负责任的 AI 资源 – Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- 微软的 FATE 研究组：[FATE：AI 中的公平性、问责性、透明性和伦理 - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

## 作业

[探索 RAI 仪表板](assignment.md)

---

**免责声明**：  
本文档使用AI翻译服务[Co-op Translator](https://github.com/Azure/co-op-translator)进行翻译。尽管我们努力确保翻译的准确性，但请注意，自动翻译可能包含错误或不准确之处。原始语言的文档应被视为权威来源。对于关键信息，建议使用专业人工翻译。我们不对因使用此翻译而产生的任何误解或误读承担责任。