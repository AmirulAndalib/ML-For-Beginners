<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9660fbd80845c59c15715cb418cd6e23",
  "translation_date": "2025-08-29T14:15:19+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "ar"
}
-->
## متطلبات مسبقة

في هذا الدرس، سنستخدم مكتبة **OpenAI Gym** لمحاكاة بيئات مختلفة. يمكنك تشغيل كود هذا الدرس محليًا (على سبيل المثال، من خلال Visual Studio Code)، وفي هذه الحالة ستفتح المحاكاة في نافذة جديدة. عند تشغيل الكود عبر الإنترنت، قد تحتاج إلى إجراء بعض التعديلات على الكود، كما هو موضح [هنا](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

في الدرس السابق، كانت قواعد اللعبة والحالة معطاة من خلال الفئة `Board` التي قمنا بتعريفها بأنفسنا. هنا سنستخدم بيئة محاكاة خاصة، والتي ستقوم بمحاكاة الفيزياء وراء توازن العصا. واحدة من أشهر بيئات المحاكاة لتدريب خوارزميات التعلم المعزز تُعرف بـ [Gym](https://gym.openai.com/)، والتي يتم صيانتها بواسطة [OpenAI](https://openai.com/). باستخدام هذه البيئة، يمكننا إنشاء بيئات مختلفة، بدءًا من محاكاة CartPole إلى ألعاب Atari.

> **ملاحظة**: يمكنك الاطلاع على بيئات أخرى متاحة من OpenAI Gym [هنا](https://gym.openai.com/envs/#classic_control).

أولاً، لنقم بتثبيت مكتبة gym واستيراد المكتبات المطلوبة (كتلة الكود 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## تمرين - تهيئة بيئة CartPole

للتعامل مع مشكلة توازن العصا، نحتاج إلى تهيئة البيئة المناسبة. كل بيئة مرتبطة بـ:

- **مساحة الملاحظة** التي تحدد بنية المعلومات التي نتلقاها من البيئة. بالنسبة لمشكلة CartPole، نتلقى موقع العصا، السرعة، وبعض القيم الأخرى.

- **مساحة الحركة** التي تحدد الإجراءات الممكنة. في حالتنا، مساحة الحركة متقطعة، وتتكون من إجراءين - **يسار** و **يمين**. (كتلة الكود 2)

1. للتهيئة، اكتب الكود التالي:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

لمعرفة كيفية عمل البيئة، دعنا نقوم بتشغيل محاكاة قصيرة لمدة 100 خطوة. في كل خطوة، نقدم أحد الإجراءات التي يجب اتخاذها - في هذه المحاكاة، نختار إجراءً عشوائيًا من `action_space`.

1. قم بتشغيل الكود أدناه وشاهد النتيجة.

    ✅ تذكر أنه يُفضل تشغيل هذا الكود على تثبيت Python محلي! (كتلة الكود 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    يجب أن ترى شيئًا مشابهًا لهذه الصورة:

    ![عصا غير متوازنة](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. أثناء المحاكاة، نحتاج إلى الحصول على الملاحظات لتحديد كيفية التصرف. في الواقع، تُرجع دالة الخطوة الملاحظات الحالية، ودالة المكافأة، وعلامة الانتهاء التي تشير إلى ما إذا كان من المنطقي متابعة المحاكاة أم لا: (كتلة الكود 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    ستنتهي برؤية شيء مثل هذا في مخرجات الدفتر:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    يحتوي متجه الملاحظة الذي يتم إرجاعه في كل خطوة من خطوات المحاكاة على القيم التالية:
    - موقع العربة
    - سرعة العربة
    - زاوية العصا
    - معدل دوران العصا

1. احصل على الحد الأدنى والحد الأقصى لتلك القيم: (كتلة الكود 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    قد تلاحظ أيضًا أن قيمة المكافأة في كل خطوة من خطوات المحاكاة دائمًا ما تكون 1. هذا لأن هدفنا هو البقاء لأطول فترة ممكنة، أي الحفاظ على العصا في وضع رأسي معقول لأطول فترة زمنية.

    ✅ في الواقع، تُعتبر محاكاة CartPole محلولة إذا تمكنا من تحقيق متوسط مكافأة قدره 195 على مدى 100 تجربة متتالية.

## تقطيع الحالة إلى قيم متقطعة

في التعلم باستخدام Q-Learning، نحتاج إلى بناء جدول Q-Table يحدد ما يجب فعله في كل حالة. للقيام بذلك، يجب أن تكون الحالة **متقطعة**، وبشكل أكثر دقة، يجب أن تحتوي على عدد محدود من القيم المتقطعة. وبالتالي، نحتاج بطريقة ما إلى **تقطيع** الملاحظات، وتحويلها إلى مجموعة محدودة من الحالات.

هناك عدة طرق يمكننا القيام بذلك:

- **التقسيم إلى فئات**. إذا كنا نعرف النطاق لقيمة معينة، يمكننا تقسيم هذا النطاق إلى عدد من **الفئات**، ثم استبدال القيمة برقم الفئة التي تنتمي إليها. يمكن القيام بذلك باستخدام طريقة [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) من مكتبة numpy. في هذه الحالة، سنعرف بدقة حجم الحالة، لأنه سيعتمد على عدد الفئات التي نختارها للتقطيع.

✅ يمكننا استخدام الاستيفاء الخطي لجلب القيم إلى نطاق محدود (على سبيل المثال، من -20 إلى 20)، ثم تحويل الأرقام إلى أعداد صحيحة عن طريق تقريبها. هذا يمنحنا تحكمًا أقل قليلاً في حجم الحالة، خاصة إذا لم نكن نعرف النطاقات الدقيقة لقيم الإدخال. على سبيل المثال، في حالتنا، 2 من أصل 4 قيم ليس لها حدود عليا/سفلى، مما قد يؤدي إلى عدد لا نهائي من الحالات.

في مثالنا، سنختار الطريقة الثانية. كما قد تلاحظ لاحقًا، على الرغم من عدم وجود حدود عليا/سفلى، فإن تلك القيم نادرًا ما تأخذ قيمًا خارج نطاقات محدودة معينة، وبالتالي ستكون الحالات ذات القيم القصوى نادرة جدًا.

1. هنا دالة تأخذ الملاحظة من نموذجنا وتنتج مجموعة من 4 قيم صحيحة: (كتلة الكود 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. دعنا نستكشف أيضًا طريقة أخرى للتقطيع باستخدام الفئات: (كتلة الكود 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. الآن دعنا نقوم بتشغيل محاكاة قصيرة ونلاحظ تلك القيم البيئية المتقطعة. لا تتردد في تجربة كل من `discretize` و `discretize_bins` وملاحظة الفرق.

    ✅ تُرجع `discretize_bins` رقم الفئة، وهو يبدأ من 0. وبالتالي، بالنسبة لقيم المتغير المدخل حول 0، تُرجع الرقم من منتصف النطاق (10). في `discretize`، لم نهتم بنطاق قيم الإخراج، مما يسمح لها بأن تكون سالبة، وبالتالي فإن القيم 0 تتوافق مع 0. (كتلة الكود 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    ✅ قم بإلغاء تعليق السطر الذي يبدأ بـ `env.render` إذا كنت تريد رؤية كيفية تنفيذ البيئة. وإلا يمكنك تنفيذها في الخلفية، وهو أسرع. سنستخدم هذا التنفيذ "غير المرئي" أثناء عملية Q-Learning.

## بنية جدول Q-Table

في درسنا السابق، كانت الحالة عبارة عن زوج بسيط من الأرقام من 0 إلى 8، وبالتالي كان من الملائم تمثيل جدول Q-Table باستخدام مصفوفة numpy ذات شكل 8x8x2. إذا استخدمنا تقطيع الفئات، فإن حجم متجه الحالة معروف أيضًا، لذا يمكننا استخدام نفس النهج وتمثيل الحالة بمصفوفة ذات شكل 20x20x10x10x2 (حيث أن 2 هو بُعد مساحة الحركة، والأبعاد الأولى تتوافق مع عدد الفئات التي اخترنا استخدامها لكل من المعلمات في مساحة الملاحظة).

ومع ذلك، في بعض الأحيان لا تكون أبعاد مساحة الملاحظة معروفة بدقة. في حالة دالة `discretize`، قد لا نكون متأكدين أبدًا من أن حالتنا تبقى ضمن حدود معينة، لأن بعض القيم الأصلية ليست محددة. وبالتالي، سنستخدم نهجًا مختلفًا قليلاً ونمثل جدول Q-Table باستخدام قاموس.

1. استخدم الزوج *(state, action)* كمفتاح للقاموس، وستكون القيمة هي القيمة المقابلة في جدول Q-Table. (كتلة الكود 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    هنا نحدد أيضًا دالة `qvalues()`، التي تُرجع قائمة بقيم جدول Q-Table لحالة معينة تتوافق مع جميع الحركات الممكنة. إذا لم تكن الإدخالات موجودة في جدول Q-Table، سنُرجع القيمة الافتراضية 0.

## لنبدأ التعلم باستخدام Q-Learning

الآن نحن جاهزون لتعليم بيتر كيفية الحفاظ على التوازن!

1. أولاً، لنقم بتحديد بعض المعاملات الفائقة: (كتلة الكود 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    هنا، `alpha` هو **معدل التعلم** الذي يحدد إلى أي مدى يجب علينا تعديل القيم الحالية لجدول Q-Table في كل خطوة. في الدرس السابق، بدأنا بـ 1، ثم قمنا بتقليل `alpha` إلى قيم أقل أثناء التدريب. في هذا المثال، سنبقيه ثابتًا فقط للتبسيط، ويمكنك تجربة تعديل قيم `alpha` لاحقًا.

    `gamma` هو **عامل الخصم** الذي يوضح إلى أي مدى يجب أن نعطي الأولوية للمكافأة المستقبلية على المكافأة الحالية.

    `epsilon` هو **عامل الاستكشاف/الاستغلال** الذي يحدد ما إذا كان يجب علينا تفضيل الاستكشاف على الاستغلال أو العكس. في خوارزميتنا، سنختار في `epsilon` بالمئة من الحالات الإجراء التالي وفقًا لقيم جدول Q-Table، وفي النسبة المتبقية من الحالات سننفذ إجراءً عشوائيًا. هذا سيسمح لنا باستكشاف مناطق من مساحة البحث التي لم نرها من قبل.

    ✅ من حيث التوازن - اختيار إجراء عشوائي (الاستكشاف) سيعمل كضربة عشوائية في الاتجاه الخاطئ، وسيتعين على العصا تعلم كيفية استعادة التوازن من تلك "الأخطاء".

### تحسين الخوارزمية

يمكننا أيضًا إجراء تحسينين على خوارزميتنا من الدرس السابق:

- **حساب متوسط المكافأة التراكمية**، على عدد من المحاكاة. سنطبع التقدم كل 5000 تكرار، وسنحسب متوسط المكافأة التراكمية خلال تلك الفترة الزمنية. هذا يعني أنه إذا حصلنا على أكثر من 195 نقطة - يمكننا اعتبار المشكلة محلولة، بجودة أعلى حتى من المطلوب.

- **حساب أقصى نتيجة تراكمية متوسطة**، `Qmax`، وسنخزن جدول Q-Table المقابل لتلك النتيجة. عندما تقوم بتشغيل التدريب، ستلاحظ أحيانًا أن المتوسط التراكمي يبدأ في الانخفاض، ونريد الاحتفاظ بقيم جدول Q-Table التي تتوافق مع أفضل نموذج تم ملاحظته أثناء التدريب.

1. اجمع جميع المكافآت التراكمية في كل محاكاة في متجه `rewards` للرسم لاحقًا. (كتلة الكود 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

ما قد تلاحظه من تلك النتائج:

- **قريب من هدفنا**. نحن قريبون جدًا من تحقيق الهدف المتمثل في الحصول على 195 مكافأة تراكمية على مدى 100+ تجربة متتالية، أو ربما قد حققنا ذلك بالفعل! حتى إذا حصلنا على أرقام أقل، لا يزال بإمكاننا عدم التأكد، لأننا نحسب المتوسط على مدى 5000 تجربة، بينما يتطلب المعيار الرسمي فقط 100 تجربة.

- **المكافأة تبدأ في الانخفاض**. أحيانًا تبدأ المكافأة في الانخفاض، مما يعني أنه يمكننا "تدمير" القيم التي تم تعلمها بالفعل في جدول Q-Table بالقيم التي تجعل الوضع أسوأ.

هذا الملاحظة تصبح أكثر وضوحًا إذا قمنا برسم تقدم التدريب.

## رسم تقدم التدريب

أثناء التدريب، قمنا بجمع قيمة المكافأة التراكمية في كل تكرار في متجه `rewards`. إليك كيف يبدو عند رسمه مقابل رقم التكرار:

```python
plt.plot(rewards)
```

![التقدم الخام](../../../../translated_images/train_progress_raw.2adfdf2daea09c596fc786fa347a23e9aceffe1b463e2257d20a9505794823ec.ar.png)

من هذا الرسم البياني، لا يمكننا استنتاج أي شيء، بسبب طبيعة عملية التدريب العشوائية التي تختلف فيها مدة جلسات التدريب بشكل كبير. لجعل هذا الرسم أكثر وضوحًا، يمكننا حساب **المتوسط المتحرك** على سلسلة من التجارب، لنقل 100. يمكن القيام بذلك بسهولة باستخدام `np.convolve`: (كتلة الكود 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![تقدم التدريب](../../../../translated_images/train_progress_runav.c71694a8fa9ab35935aff6f109e5ecdfdbdf1b0ae265da49479a81b5fae8f0aa.ar.png)

## تغيير المعاملات الفائقة

لجعل التعلم أكثر استقرارًا، من المنطقي تعديل بعض المعاملات الفائقة أثناء التدريب. على وجه الخصوص:

- **بالنسبة لمعدل التعلم**، `alpha`، يمكننا البدء بقيم قريبة من 1، ثم نستمر في تقليل هذا المعامل. مع مرور الوقت، سنحصل على قيم احتمالية جيدة في جدول Q-Table، وبالتالي يجب علينا تعديلها بشكل طفيف، وليس استبدالها بالكامل بقيم جديدة.

- **زيادة epsilon**. قد نرغب في زيادة `epsilon` ببطء، من أجل تقليل الاستكشاف وزيادة الاستغلال. ربما يكون من المنطقي البدء بقيمة منخفضة لـ `epsilon`، ثم زيادتها تدريجيًا لتقترب من 1.
> **المهمة 1**: جرّب تعديل قيم المعاملات الفائقة (Hyperparameters) وانظر إذا كان بإمكانك تحقيق مكافأة تراكمية أعلى. هل تحصل على أكثر من 195؟
> **المهمة 2**: لحل المشكلة بشكل رسمي، تحتاج إلى تحقيق متوسط مكافأة قدره 195 عبر 100 تشغيل متتالي. قم بقياس ذلك أثناء التدريب وتأكد من أنك قد حللت المشكلة بشكل رسمي!

## رؤية النتيجة عمليًا

سيكون من المثير للاهتمام رؤية كيف يتصرف النموذج المدرب بالفعل. دعنا نقوم بتشغيل المحاكاة ونتبع نفس استراتيجية اختيار الإجراءات كما في التدريب، حيث يتم أخذ العينات وفقًا لتوزيع الاحتمالات في Q-Table: (كتلة الكود 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

يجب أن ترى شيئًا مثل هذا:

![عربة متوازنة](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## 🚀التحدي

> **المهمة 3**: هنا، كنا نستخدم النسخة النهائية من Q-Table، والتي قد لا تكون الأفضل. تذكر أننا قمنا بتخزين أفضل نسخة أداء من Q-Table في المتغير `Qbest`! جرب نفس المثال باستخدام أفضل نسخة أداء من Q-Table عن طريق نسخ `Qbest` إلى `Q` وشاهد إذا لاحظت الفرق.

> **المهمة 4**: هنا لم نكن نختار أفضل إجراء في كل خطوة، بل كنا نأخذ العينات وفقًا لتوزيع الاحتمالات المقابل. هل سيكون من المنطقي دائمًا اختيار أفضل إجراء، الذي يحتوي على أعلى قيمة في Q-Table؟ يمكن القيام بذلك باستخدام وظيفة `np.argmax` لمعرفة رقم الإجراء الذي يتوافق مع أعلى قيمة في Q-Table. قم بتنفيذ هذه الاستراتيجية وشاهد إذا كانت تحسن التوازن.

## [اختبار ما بعد المحاضرة](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/48/)

## الواجب
[تدريب سيارة جبلية](assignment.md)

## الخاتمة

لقد تعلمنا الآن كيفية تدريب الوكلاء لتحقيق نتائج جيدة فقط من خلال توفير دالة مكافأة تحدد الحالة المطلوبة للعبة، ومنحهم فرصة لاستكشاف مساحة البحث بذكاء. لقد طبقنا بنجاح خوارزمية Q-Learning في حالات البيئات المنفصلة والمستمرة، ولكن مع إجراءات منفصلة.

من المهم أيضًا دراسة الحالات التي تكون فيها حالة الإجراء مستمرة، وعندما تكون مساحة الملاحظة أكثر تعقيدًا، مثل الصورة من شاشة لعبة أتاري. في تلك المشاكل، غالبًا ما نحتاج إلى استخدام تقنيات تعلم الآلة الأكثر قوة، مثل الشبكات العصبية، لتحقيق نتائج جيدة. هذه المواضيع الأكثر تقدمًا هي موضوع دورتنا القادمة الأكثر تقدمًا في الذكاء الاصطناعي.

---

**إخلاء المسؤولية**:  
تمت ترجمة هذا المستند باستخدام خدمة الترجمة الآلية [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية هو المصدر الموثوق. للحصول على معلومات حساسة أو هامة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة تنشأ عن استخدام هذه الترجمة.